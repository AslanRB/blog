<!DOCTYPE html>
<html lang="en-us">
    <head>
        

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>FreqStats 1</title>
        
        <style>

    html body {
        font-family: 'Alegreya Sans', sans-serif;
        background-color: white;
    }

    :root {
        --accent: black;
        --border-width:  5px ;
    }

</style>


<link rel="stylesheet" href="/css/main.css">





<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alegreya%20Sans">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
 

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
    
    <script>hljs.initHighlightingOnLoad();</script>






<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>
 <meta name="generator" content="Hugo 0.76.5" />
        

        

        
            <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        

        

    </head>

    <body>
        

        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container">
                <div class="navbar-header">
                    <a class="navbar-brand visible-xs" href="#">FreqStats 1</a>
                    <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                </div>
                <div class="collapse navbar-collapse">
                    
                        <ul class="nav navbar-nav">
                            
                                <li><a href="/">Home</a></li>
                            
                                <li><a href="/post/">Posts</a></li>
                            
                                <li><a href="/about/">About</a></li>
                            
                        </ul>
                    
                    
                        <ul class="nav navbar-nav navbar-right">
                            
                                <li class="navbar-icon"><a href="mailto:a.bellmann@yahoo.de"><i class="fa fa-envelope-o"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://twitter.com/aslanbellmann/"><i class="fa fa-twitter"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://github.com/AslanRB/"><i class="fa fa-github"></i></a></li>
                            
                        </ul>
                    
                </div>
            </div>
        </nav>


<main>

    <div>
        <h2>FreqStats 1</h2>
        <h5>December 6, 2020</h5>
        

    </div>

    <div align="start" class="content">


<p>Usually most of the information is in the text, and plots are used to highlight the key information. Not in this. Here, I tried to put ALL the informaton in the plots. For the most part, you could just go from plot to plot, looking at each one until you understand what is going on, and you got all the information – and I encourage you to do so. If it doesn’t work, the information is also in the text. Bla</p>
<table>
<thead>
<tr class="header">
<th>Truth ———</th>
<th>Decision ———-</th>
<th>Event ———————-</th>
<th>Rate ——–</th>
<th>Also known as ———–</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mu_0 = \mu\)</span></td>
<td>Reject</td>
<td>False positive error</td>
<td><span class="math inline">\(\alpha\)</span></td>
<td>‘significance threshold’</td>
</tr>
<tr class="even">
<td></td>
<td>Don’t Reject</td>
<td>True negative</td>
<td><span class="math inline">\(1-\alpha\)</span></td>
<td>‘coverage probability’</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mu_0 \ne \mu\)</span></td>
<td>Reject</td>
<td>True positive</td>
<td><span class="math inline">\(1-\beta\)</span></td>
<td>‘statistical power’</td>
</tr>
<tr class="even">
<td></td>
<td>Don’t Reject</td>
<td>False negative error</td>
<td><span class="math inline">\(\beta\)</span></td>
<td></td>
</tr>
</tbody>
</table>
<div id="probability-in-frequentist-statistics" class="section level1">
<h1>Probability in Frequentist Statistics</h1>
<p>Mathematically, probability is just some quantitiy defined through the <em>Kolmogorov axioms</em>. From these axioms, we can derive further mathematical rules of probability. So far so good. However, once we expand our horizons beyond the abstract definition to the real-world application, we suddenly have to deal with multiple interpretations of probability. This can cause a great deal of confusion, so let’s be clear about what interpretation of probability we will use throughout this blogpost…</p>
<p>The name gives it away: In frequentist statistics, probabilities refer to <em>frequencies</em>, aka <em>rates</em>. That is, the probability that an event produces an outcome is the frequency (rate) at which that event produces that outcome if the event is repeated infinitely many times. And because I don’t want to repeat the phrase ‘if the event is repeated infinitely many times’ itself infinitely many times, I will from now on just say <em>in the long-run</em>. So, the probability that an event produces an outcome is the frequecy (rate) at which that event produces that outcome in the long-run.</p>
<p>For example, what is the probability that flipping a coin will get you tails? The figure below shows the proportion of tails you have observed after each coin flip, for the first 100 flips. As the number of flips tends towards infinity, the proportion of tails converges on 0.5. Thus, the probability of getting tails is 0.5 – you’ll get tails 50% of the time in the long-run. (Below, the proportion of tails already seems to converge on 0.5 with the first 100 flips, but this is not reliable. To get a reliable estimate of the probability, we’d need more like 10,000+ repetitions).</p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/coin%20flip%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Under the <em>frequentist</em> account of probability, we can only distribute probability over outcomes of events that are (in principle) repeatable – such as whether a coin flip will yield tails, or whether three cards that you draw from a well-shuffled deck will have the same color. We cannot assign probability to non-repeatable events – such as whether time travel is possible, or whether it will rain tomorrow. Under an <em>epistemic</em> account of probability, where probability may quantify a rational agent’s degree of belief, we absolutely could assign probability to non-repeatable events. But not under the <em>frequentist</em> account.</p>
<p>To avoid confusion, I will just use the words ‘rate’ or ‘frequency’ instead of ‘probability’ – at least most of the time.</p>
</div>
<div id="the-working-scenario" class="section level1">
<h1>The Working Scenario</h1>
<p>We are interested in some continuous variable, <span class="math inline">\(X\)</span> – if you prefer you can mentally insert something specific such as height, weight, or age. We will assume that <span class="math inline">\(X\)</span> is normally distributed within the population with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, i.e. <span class="math inline">\(X \sim {\sf Normal}(\mu,\sigma)\)</span>. We are interested in the population mean <span class="math inline">\(\mu\)</span>. For now, we will assume that we know the population standard deviation <span class="math inline">\(\sigma\)</span> (this is pretty much never the case in reality, but it makes things easier to explain). Unfortunately, we cannot obtain the precise value of <span class="math inline">\(\mu\)</span> by simply measuring the whole population. However, we can draw a random sample from the population and use it to construct an interval that should contain <span class="math inline">\(\mu\)</span>.</p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/overview%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>More specifically, we draw a random sample of size <span class="math inline">\(N\)</span>. We can measure the sample and calculate the sample mean <span class="math inline">\(m\)</span> and the sample standard deviation <span class="math inline">\(s\)</span>. Remember, our goal is to estimate <span class="math inline">\(\mu\)</span>. We do this by rejecting all hypothetical values of <span class="math inline">\(\mu\)</span> that are incompatible with our sample data (<span class="math inline">\(m\)</span> and <span class="math inline">\(s\)</span>), thereby obtaining an interval of non-rejected values of <span class="math inline">\(\mu\)</span>.</p>
<p>I want to emphasize that frequentist statistics is really all about rejection. Imagine you are at the supermarket and you want to buy an apple. However, not all the apples are of the same quality – and of course you want the single best one available. You start visually inspecting all the apples, rejecting the ones that don’t look fresh. You end up with a bunch of non-rejected apples. Among them is the best available apple – but you don’t know which one exactly; you have just narrowed it down.</p>
<p>This is pretty much what we are doing. Instead of the best available apple, we care about the true value of <span class="math inline">\(\mu\)</span>. Instead of rejecting apples, we reject hypothetical values of <span class="math inline">\(\mu\)</span>. Instead of apple aesthetics, we use incompatibility with our sample data as a rejection criterion. Now, the obvious question is: What does ‘incompatibility with our sample data’ mean? How exactly do we construct the interval?</p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/overview%20plot%20estimation-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="sampling-distributions" class="section level1">
<h1>Sampling Distributions</h1>
<p>To get there, we need to understand what a sampling distribution is. We start with a population distribution, from which we repeatedly draw samples of size <span class="math inline">\(N\)</span>. Each time we draw a sample, we calculate and plot the sample mean <span class="math inline">\(m\)</span>. As the <span class="math inline">\(m\)</span>’s accumulate, they form a distribution themselves. In the limit where the number of samples we draw (and thus the number of <span class="math inline">\(m\)</span>’s we plot) tends towards infinity, this distribution is the sampling distribution of the sample mean <span class="math inline">\(m\)</span>.</p>
<p><em>Aside: Sampling distributions exist for any sample statistic. For example, instead of the mean <span class="math inline">\(m\)</span>, we could calculate and plot the standard deviation <span class="math inline">\(s\)</span> for each sample. As the number of samples we draw (and thus the number of <span class="math inline">\(s\)</span>’s) tends towards infinity, we obtain the sampling distribution of the sample standard deviation <span class="math inline">\(s\)</span>.</em></p>
<p>The sampling distribution of the sample mean <span class="math inline">\(m\)</span> is illustrated below, for three different sample sizes <span class="math inline">\(N\)</span>. The top panel shows the first five samples and their sample means <span class="math inline">\(m\)</span>. The bottom panel shows the distributions of <span class="math inline">\(m\)</span> from 100,000 samples (a good approximation of what the distribution looks like as the number of samples tends towards infinity).</p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/sampdist%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>As you can see in the plot, the sampling distribution of <span class="math inline">\(m\)</span> has three key properties: <span class="math inline">\(m \sim {\sf Normal}(\mu,\frac{\sigma}{\sqrt{N}})\)</span>.</p>
<ol style="list-style-type: decimal">
<li>The distribution of <span class="math inline">\(m\)</span> is normal,</li>
<li>the mean of the distribution is the population mean <span class="math inline">\(\mu\)</span>,</li>
<li>the standard deviation of the distribution is <span class="math inline">\(\frac{\sigma}{\sqrt{N}}\)</span> (aka the standard error of the mean), and thus decreases as <span class="math inline">\(N\)</span> increases.</li>
</ol>
<p><em>Aside: The above points hold true as long as the population distribution has a mean <span class="math inline">\(\mu\)</span> and finite variance <span class="math inline">\(\sigma^2\)</span> – the population distribution doesn’t have to be a normal distribution as in our example.</em></p>
<p>With that, let’s see how we can use the sampling distribution of the mean to estimate <span class="math inline">\(\mu\)</span>.</p>
</div>
<div id="p-values" class="section level1">
<h1><span class="math inline">\(p\)</span>-values</h1>
<p>Recall that we reject hypothetical values for <span class="math inline">\(\mu\)</span> if they are incompatible with our sample data. So how do we quantify (in)compatibility? Via the <span class="math inline">\(p\)</span>-value! The <span class="math inline">\(p\)</span>-value can take any number from 0 to 1. The lower the <span class="math inline">\(p\)</span>-value for a given hypothetical value for <span class="math inline">\(\mu\)</span>, the less compatible that hypothetical value for <span class="math inline">\(\mu\)</span> is with our sample data.</p>
<p>In what follows, we will draw a sample of <span class="math inline">\(N=15\)</span> and compute p-values using the corresponding sampling distribution of <span class="math inline">\(m\)</span>. We will compute three types of p-values: right-sided, left-sided, and two-sided. I tried to make the plots pretty self-explanatory, so I encourage you to look at each plot, try to figure out what’s going on, and then read my comments.</p>
<div id="computing-and-interpreting-the-right-sided-p-value" class="section level3">
<h3>Computing and interpreting the right-sided <span class="math inline">\(p\)</span>-value</h3>
<p>We start with the right-sided <span class="math inline">\(p\)</span>-value. Check out the plot below…</p>
<p>The <strong>top two panels</strong> show our building blocks:</p>
<ol style="list-style-type: decimal">
<li>The sampling distribution of <span class="math inline">\(m\)</span> (red) for our sample size <span class="math inline">\(N\)</span>.</li>
<li>The sampling distribution of <span class="math inline">\(m\)</span> (red) and the population distribution (grey) have a common mean <span class="math inline">\(\mu\)</span>.</li>
<li>Our sample mean <span class="math inline">\(m\)</span> (red square with black solid vertical line).</li>
</ol>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/rightsided%20pvalue%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>The <strong>next three panels</strong> show how right-sided <span class="math inline">\(p\)</span>-values are computed for three hypothetical values of <span class="math inline">\(\mu\)</span>. Here are the two steps:</p>
<ol style="list-style-type: decimal">
<li>We center the sampling distribution of <span class="math inline">\(m\)</span> over the hypothetical value of <span class="math inline">\(\mu\)</span> (blue dashed line). By doing so, we assume that the value of <span class="math inline">\(\mu\)</span> is indeed that hypothetical value (blue dashed line) – see building block 2 above.</li>
<li>The <em>right</em>-sided <span class="math inline">\(p\)</span>-value is the area of the sampling distribution of <span class="math inline">\(m\)</span> that is <em>right</em> to our sample mean <span class="math inline">\(m\)</span> (black solid line). Recall that the sampling distribution of <span class="math inline">\(m\)</span> represents a ‘number-approaching-infinity’ amount of <span class="math inline">\(m\)</span>’s, each obtained from a random sample of size <span class="math inline">\(N\)</span>. Thus, the area <em>right</em> to our sample mean <span class="math inline">\(m\)</span> tells us how often we would observe an <span class="math inline">\(m\)</span> <em>at least as large</em> as ours, if we drew ‘number-approaching-infinity’ samples of size <span class="math inline">\(N\)</span>.</li>
</ol>
<p>That is, the <em>right</em>-sided <span class="math inline">\(p\)</span>-value for a hypothetical value of <span class="math inline">\(\mu\)</span> means: If <span class="math inline">\(\mu\)</span> were indeed this hypothetical value, we would observe a sample mean <span class="math inline">\(m\)</span> <em>at least as large</em> as the one we observed in our sample <span class="math inline">\(p \times 100 \%\)</span> of the time – if we drew ‘number-approaching-infinity’ samples of size <span class="math inline">\(N\)</span>.</p>
<p>The <strong>bottom panel</strong> shows <em>right</em>-sided <span class="math inline">\(p\)</span>-values for a range of hypothetical values of <span class="math inline">\(\mu\)</span> (often referred to as a <span class="math inline">\(p\)</span>-value curve).</p>
</div>
<div id="computing-and-interpreting-the-left-sided-p-value" class="section level3">
<h3>Computing and interpreting the left-sided <span class="math inline">\(p\)</span>-value</h3>
<p>Let’s move on to the left-sided <span class="math inline">\(p\)</span>-value. Check out the plot below…</p>
<p>The <strong>top two panels</strong> show our building blocks (same as before):</p>
<ol style="list-style-type: decimal">
<li>The sampling distribution of <span class="math inline">\(m\)</span> (red) for our sample size <span class="math inline">\(N\)</span>.</li>
<li>The sampling distribution of <span class="math inline">\(m\)</span> (red) and the population distribution (grey) have a common mean <span class="math inline">\(\mu\)</span>.</li>
<li>Our sample mean <span class="math inline">\(m\)</span> (red square with black solid vertical line).</li>
</ol>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/leftsided%20pvalue%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>The <strong>next three panels</strong> show how right-sided <span class="math inline">\(p\)</span>-values are computed for (the same) three hypothetical values of <span class="math inline">\(\mu\)</span>. Here are the two steps:</p>
<ol style="list-style-type: decimal">
<li>We center the sampling distribution of <span class="math inline">\(m\)</span> over the hypothetical value of <span class="math inline">\(\mu\)</span> (blue dashed line). By doing so, we assume that the value of <span class="math inline">\(\mu\)</span> is indeed that hypothetical value (blue dashed line) – see building block 2 above.</li>
<li>The <em>left</em>-sided <span class="math inline">\(p\)</span>-value is the area of the sampling distribution of <span class="math inline">\(m\)</span> that is <em>left</em> to our sample mean <span class="math inline">\(m\)</span> (black solid line). Recall that the sampling distribution of <span class="math inline">\(m\)</span> represents a ‘number-approaching-infinity’ amount of <span class="math inline">\(m\)</span>’s, each obtained from a random sample of size <span class="math inline">\(N\)</span>. Thus, the area <em>left</em> to our sample mean <span class="math inline">\(m\)</span> tells us how often we would observe an <span class="math inline">\(m\)</span> <em>at least as small</em> as ours, if we drew ‘number-approaching-infinity’ samples of size <span class="math inline">\(N\)</span>.</li>
</ol>
<p>That is, the <em>left</em>-sided <span class="math inline">\(p\)</span>-value for a hypothetical value of <span class="math inline">\(\mu\)</span> means: If <span class="math inline">\(\mu\)</span> were indeed this hypothetical value, we would observe a sample mean <span class="math inline">\(m\)</span> <em>at least as small</em> as the one we observed in our sample <span class="math inline">\(p \times 100 \%\)</span> of the time – if we drew ‘number-approaching-infinity’ samples of size <span class="math inline">\(N\)</span>.</p>
<p>The <strong>bottom panel</strong> shows <em>left</em>-sided <span class="math inline">\(p\)</span>-values for a range of hypothetical values of <span class="math inline">\(\mu\)</span> (often referred to as a <span class="math inline">\(p\)</span>-value curve). Note how the <em>left</em>-sided <span class="math inline">\(p\)</span>-value curve and the <em>right</em>-sided <span class="math inline">\(p\)</span>-value curve are horizontal mirror images.</p>
</div>
<div id="computing-and-interpreting-the-two-sided-p-value" class="section level3">
<h3>Computing and interpreting the two-sided <span class="math inline">\(p\)</span>-value</h3>
<p>Let’s move on to the two-sided <span class="math inline">\(p\)</span>-value. Check out the plot below…</p>
<p>The <strong>top two panels</strong> show our building blocks (same as before):</p>
<ol style="list-style-type: decimal">
<li>The sampling distribution of <span class="math inline">\(m\)</span> (red) for our sample size <span class="math inline">\(N\)</span>.</li>
<li>The sampling distribution of <span class="math inline">\(m\)</span> (red) and the population distribution (grey) have a common mean <span class="math inline">\(\mu\)</span>.</li>
<li>Our sample mean <span class="math inline">\(m\)</span> (red square with black solid vertical line).</li>
</ol>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/twosided%20pvalue%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>The <strong>next three panels</strong> show how two-sided <span class="math inline">\(p\)</span>-values are computed for the same three hypothetical values of <span class="math inline">\(\mu\)</span> as before. Namely, both a left-sided <span class="math inline">\(p\)</span>-value and a right-sided <span class="math inline">\(p\)</span>-value are computed, the minimum of the two is taken, and then multiplied by two because we used the sampling distribution twice. Here are the two steps:</p>
<ol style="list-style-type: decimal">
<li>We center the sampling distribution of <span class="math inline">\(m\)</span> over the hypothetical value of <span class="math inline">\(\mu\)</span> (blue dashed line). By doing so, we assume that the value of <span class="math inline">\(\mu\)</span> is indeed that hypothetical value (blue dashed line) – see building block 2 above.</li>
<li>The <em>two</em>-sided <span class="math inline">\(p\)</span>-value is <em>twice</em> the area of the sampling distribution of <span class="math inline">\(m\)</span> that is <em>left or right – whichever area is smaller –</em> to our sample mean <span class="math inline">\(m\)</span> (black solid line). As you can see in the plot, the smaller area is always on the <em>other</em> side of our sample mean <span class="math inline">\(m\)</span>, <em>relative to the hypothetical value of <span class="math inline">\(\mu\)</span>.</em> Recall that the sampling distribution of <span class="math inline">\(m\)</span> represents a ‘number-approaching-infinity’ amount of <span class="math inline">\(m\)</span>’s, each obtained from a random sample of size <span class="math inline">\(N\)</span>. Thus, the area on the <em>other</em> side to our sample mean <span class="math inline">\(m\)</span> tells us how often we would observe an <span class="math inline">\(m\)</span> <em>at least as far away from <span class="math inline">\(\mu\)</span></em> as ours, if we drew ‘number-approaching-infinity’ samples of size <span class="math inline">\(N\)</span>.</li>
</ol>
<p>That is, the <em>two</em>-sided <span class="math inline">\(p\)</span>-value for a hypothetical value of <span class="math inline">\(\mu\)</span> means: If <span class="math inline">\(\mu\)</span> were indeed this hypothetical value, we would observe a sample mean <span class="math inline">\(m\)</span> <em>at least as far away from <span class="math inline">\(\mu\)</span></em> as the one we observed in our sample <span class="math inline">\(p \times 100 \%\)</span> of the time – if we drew ‘number-approaching-infinity’ samples of size <span class="math inline">\(N\)</span>.</p>
<p>The <strong>bottom panel</strong> shows <em>two</em>-sided <span class="math inline">\(p\)</span>-values for a range of hypothetical values of <span class="math inline">\(\mu\)</span> (often referred to as a <span class="math inline">\(p\)</span>-value curve).</p>
</div>
<div id="interpreting-p-values-summary" class="section level3">
<h3>Interpreting <span class="math inline">\(p\)</span>-values (summary)</h3>
<p>The <span class="math inline">\(p\)</span>-value is a measure of compatibility of a hypothetical value of <span class="math inline">\(\mu\)</span> with our sample data.</p>
<p>In our working scenario, our sample data is summarized by the sample mean <span class="math inline">\(m\)</span> because we know the sampling distribution of <span class="math inline">\(m\)</span>. Thus, the <span class="math inline">\(p\)</span>-value answers the question: If <span class="math inline">\(\mu\)</span> were indeed this hypothetical value, how often would we observe a sample mean <span class="math inline">\(m\)</span> that is <em>at least</em> as large (right-sided) / as small (left-sided) / as far away from <span class="math inline">\(\mu\)</span> (two-sided) as the one we observed if we repeated the experiment ‘number-approaching-infinity’ times? The less often, the less compatible the hypothetical value of <span class="math inline">\(\mu\)</span> is with our sample data.</p>
<p>In our working scenario, the sample data can be summarized by the sample mean <span class="math inline">\(m\)</span> because we know the corresponding sampling distribution <span class="math inline">\(m \sim {\sf Normal}(\mu,\frac{\sigma}{\sqrt{N}})\)</span> under some assumed value for <span class="math inline">\(\mu\)</span> – as we know <span class="math inline">\(\sigma\)</span>. If we didn’t, we’d have to summarize our sample data using some other statistic whose sampling distribution we do know under some assumed true value for our parameter of interest. After all, we need the sampling distribution to compute <span class="math inline">\(p\)</span>-values.</p>
<p>In general, our data is summarized by some statistic whose sampling distribution we know. The <span class="math inline">\(p\)</span>-value answers the question: If our parameter of interest were indeed this hypothetical value, how often would we observe a statistic that is <em>at least</em> as large (right-sided) / as small (left-sided) / as extreme (two-sided) as the one we observed if we repeated the experiment ‘number-approaching-infinity’ times?</p>
</div>
<div id="avoid-misinterpreting-p-values" class="section level3">
<h3>Avoid misinterpreting <span class="math inline">\(p\)</span>-values</h3>
<p>A common mistake is to say that the <span class="math inline">\(p\)</span>-value is the probability that the hypothetical value of <span class="math inline">\(\mu\)</span> is indeed the true value. Let’s put aside that this would violate the mathematical laws of probability theory (the probability over all hypothetical values of <span class="math inline">\(\mu\)</span> would not add up to 1 – in fact, it would be infinite because there are infinitely many hypothetical values for <span class="math inline">\(\mu\)</span>, as it is an element of the space of <em>Real numbers</em>). The statement immediately raises a red flag: We would be assigning probability to a non-repeatable event, which is inconsistent with the frequentist interpretation of probability!</p>
<p>And if we remember the computational steps, it is clear why the <span class="math inline">\(p\)</span>-value is not the probability that the hypothetical value of <span class="math inline">\(\mu\)</span> is indeed the true value: the <span class="math inline">\(p\)</span>-value is computed <em>under the assumption that the hypothetical value of <span class="math inline">\(\mu\)</span> is indeed the true value</em>.</p>
<p>To drive this point home, consider the following example: You have a big urn filled with red and blue pills – and you want to know the red/blue ratio. You grab a 10 pills and look at the ratio: it’s 7/3 red/blue. How would you obtain a <span class="math inline">\(p\)</span>-value for a hypothetical true ratio of 50%/50%, empirically? First, <em>you fill a separate identical urn with 50%/50% red/blue pills</em>. Then, you grab 10 pills over and over again (each time putting them back and shaking the urn to mix the pills so your picks are random). The rate at which you get ratios of 7/3 or higher is your right-sided <span class="math inline">\(p\)</span>-value. The rate at which you get ratios of 7/3 or lower is your left-sided <span class="math inline">\(p\)</span>-value. The minimum of those two (here: the right-sided <span class="math inline">\(p\)</span>-value) multiplied by 2 is your two-sided <span class="math inline">\(p\)</span>-value. All these <span class="math inline">\(p\)</span>-values for a hypothetical ratio of 50%/50% are <em>based on an urn where the ratio is indeed 50%/50%</em>. <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>The probability <span class="math inline">\(p\)</span> does not refer to the hypothetical value of <span class="math inline">\(\mu\)</span> being the true one – it refers to how often, in the long run, we’d observe <span class="math inline">\(m\)</span> as (small / large / far away from <span class="math inline">\(\mu\)</span> as ours) <em>if</em> the hypothetical value of <span class="math inline">\(\mu\)</span> <em>were</em> the true one.</p>
</div>
<div id="distribution-of-p-values" class="section level3">
<h3>Distribution of <span class="math inline">\(p\)</span>-values</h3>
<p>If we repeatedly draw samples and each time compute the <span class="math inline">\(p\)</span>-value for the <em>same</em> hypothetical vlaue of <span class="math inline">\(\mu\)</span>, how will those <span class="math inline">\(p\)</span>-values be distributed? If the hypothetical value of <span class="math inline">\(\mu\)</span> that we are evaulating is indeed the true value, then <span class="math inline">\(p\)</span>-values will be uniformly distributed – i.e. <span class="math inline">\(p \sim {\sf Uniform}(0,1)\)</span>. This means that all <span class="math inline">\(p\)</span>-values (between 0 and 1) are equally likely in the long run.</p>
<p>I will now explain why this is the case using left-sided <span class="math inline">\(p\)</span>-values, but the same reasoning also applies to right-sided and two-sided <span class="math inline">\(p\)</span>-values.</p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/punif%20explained%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>First, let’s be clear about what we are doing: We repeatedly draw samples of size <span class="math inline">\(N\)</span>. Each time we compute a left-sided <span class="math inline">\(p\)</span>-value for the same hypothetical value of <span class="math inline">\(\mu\)</span>. This hypothetical value of <span class="math inline">\(\mu\)</span> is indeed the true value (this is highlighted above by centering the sampling distribution of <span class="math inline">\(m\)</span> at the population mean <span class="math inline">\(\mu\)</span>). Each time we draw a sample, we will get a different <span class="math inline">\(m\)</span> (black solid line) and thus a different <span class="math inline">\(p\)</span>-value (red area left to the black solid line). Above are two examples which happen to be <span class="math inline">\(p=0.2\)</span> and <span class="math inline">\(p=0.8\)</span>.</p>
<p>Now, we ask: How often would we observe a <span class="math inline">\(p\)</span>-value at least as small as ours? For that to happen, <span class="math inline">\(m\)</span> must be at least as far left as ours. Thus, the answer is given by the red area left to the black solid line – which is the <span class="math inline">\(p\)</span>-value! We would observe <span class="math inline">\(p \le 0.2\)</span> at a rate of <span class="math inline">\(0.2\)</span> and <span class="math inline">\(p \le 0.8\)</span> at a rate of <span class="math inline">\(0.8\)</span>! In general, we would observe <span class="math inline">\(p \le P\)</span> at a rate <span class="math inline">\(P\)</span> and equivalently <span class="math inline">\(P_1 \le p \le P_2\)</span> at a rate <span class="math inline">\(P_2 - P_1\)</span> (e.g. <span class="math inline">\(0.2 \le p \le 0.8\)</span> at a rate of <span class="math inline">\(0.6\)</span>). Thus, if the hypothetical value of <span class="math inline">\(\mu\)</span> that we are evaulating is indeed the true value, then <span class="math inline">\(p\)</span>-values will be uniformly distributed (and if <em>not</em>, then <em>not</em>).</p>
</div>
</div>
<div id="rejection-if-p-le-alpha" class="section level1">
<h1>Rejection if <span class="math inline">\(p \le \alpha\)</span></h1>
<p>Recall that we reject hypothetical values for <span class="math inline">\(\mu\)</span> if they are incompatible with our sample data. Now we know that we quantify compatibility via the <span class="math inline">\(p\)</span>-value, so we can impose a threshold <span class="math inline">\(\alpha\)</span> that defines how low the compatibility has to be for us to reject the hypothetical value for <span class="math inline">\(\mu\)</span>. In other words, if <span class="math inline">\(p \le \alpha\)</span>, we reject the hypothetical value for <span class="math inline">\(\mu\)</span>.</p>
<div id="our-rule-of-inference" class="section level3">
<h3>Our Rule of Inference</h3>
<p>In propositional calculus, this rule of inference is called <em>modus tollens</em>.</p>
<p>Imagine you are looking for me in a group a people. You check every person in that group. However, all you know is that I am always wearing a blue shirt. It goes like this:</p>
<ol style="list-style-type: decimal">
<li><strong>If I then B</strong>: If you see me, I’ll always be wearing a blue shirt.</li>
<li><strong>Not B</strong>: The person you see is not wearing a blue shirt.</li>
<li><strong>Therefore not I</strong>: The person you see is not me.</li>
</ol>
<ul>
<li>Note that <strong>you cannot say B therefore I</strong>: Just because you see someone wearing a blue shirt doesn’t mean it’s me; many people wear blue shirts.</li>
</ul>
<p>Will you ever make an error – i.e. you indeed see me, but conclude that you are not? Never – the method is bulletproof!</p>
<p>Now consider this slight modification:</p>
<ol style="list-style-type: decimal">
<li><strong>If I then B <em>99% of the time</em></strong>: If you see me, I’ll be wearing a blue shirt <em>99% of the time</em>.</li>
<li><strong>Not B</strong>: The person you see is not wearing a blue shirt.</li>
<li><strong>Therefore not I</strong>: The person you see is not me.</li>
</ol>
<ul>
<li>Note that <strong>you cannot say B therefore I</strong>: Just because you see someone wearing a blue shirt doesn’t mean it’s me; many people wear blue shirts.</li>
</ul>
<p>Will you ever make an error – i.e. you indeed see me, but conclude that you are not? Yes, 1% of the time – the method has an error rate of 0.01!</p>
<p>Now let’s go back to our working scenario. Instead of rejecting that the person you see is me, we reject that the hypothetical value for <span class="math inline">\(\mu\)</span> is the true value. Instead of using the shirt color you’d expect most of the time <em>if</em> the person was me, we are using the <span class="math inline">\(p\)</span>-value or equivalently the sample mean <span class="math inline">\(m\)</span> that we’d expect most of the time <em>if</em> the hypothetical value for <span class="math inline">\(\mu\)</span> was the true value.</p>
<p>Phrasing it in terms of the <span class="math inline">\(p\)</span>-value:</p>
<ol style="list-style-type: decimal">
<li>If this hypothetical value for <span class="math inline">\(\mu\)</span> were the true value, we would observe <span class="math inline">\(p &gt; \alpha\)</span> with frequency <span class="math inline">\(1-\alpha\)</span> (and <span class="math inline">\(p \le \alpha\)</span> with frequency <span class="math inline">\(\alpha\)</span>). (<em>Because then, <span class="math inline">\(p\)</span>-values would be uniformly distributed!</em>)</li>
<li>We did not observe <span class="math inline">\(p &gt; \alpha\)</span> (we observed <span class="math inline">\(p \le \alpha\)</span>).</li>
<li>Therefore this hypothetical value for <span class="math inline">\(\mu\)</span> is not the true value. We reject it!</li>
</ol>
<ul>
<li>Note that we <em>cannot</em> say: We did observe <span class="math inline">\(p &gt; \alpha\)</span>, therefore this hypothetical value for <span class="math inline">\(\mu\)</span> is the true value. Many hypothetical values for <span class="math inline">\(\mu\)</span> will have <span class="math inline">\(p &gt; \alpha\)</span>.</li>
</ul>
<p>Equivalently, phrasing it in terms of the sample mean <span class="math inline">\(m\)</span>:</p>
<ol style="list-style-type: decimal">
<li>If this hypothetical value for <span class="math inline">\(\mu\)</span> were the true value, <span class="math inline">\(m\)</span> would fall within a certain range with frequency <span class="math inline">\(1-\alpha\)</span> (and <span class="math inline">\(m\)</span> would fall outside that range with frequency <span class="math inline">\(\alpha\)</span>).</li>
<li>We did not observe <span class="math inline">\(m\)</span> within that range (we observed <span class="math inline">\(m\)</span> outside that range).</li>
<li>Therefore this hypothetical value for <span class="math inline">\(\mu\)</span> is not the true value. We reject it!</li>
</ol>
<ul>
<li>Note that we <em>cannot</em> say: We did observe <span class="math inline">\(m\)</span> within that range, therefore this hypothetical value for <span class="math inline">\(\mu\)</span> is the true value. <span class="math inline">\(m\)</span> will be within the anticipated range for many hypothetical values of <span class="math inline">\(\mu\)</span>.</li>
</ul>
<p>Will we ever make an error – i.e. our hypothetical value for <span class="math inline">\(\mu\)</span> is indeed the true value, yet we reject it? Yes, <span class="math inline">\(\alpha \times 100\%\)</span> of the time – the method has an error rate of <span class="math inline">\(\alpha\)</span>! Note that the aforementioned error is called a <em>false positive error</em>. Thus, <span class="math inline">\(\alpha\)</span> is called the <em>false positive error rate</em>.</p>
</div>
</div>
<div id="confidence-intervals" class="section level1">
<h1>Confidence Intervals</h1>
<p>So, we reject hypothetical values for <span class="math inline">\(\mu\)</span> if <span class="math inline">\(p \le \alpha\)</span>. We can easily compute an interval of non-rejected values for <span class="math inline">\(\mu\)</span> – i.e. an interval containing all hypothetical values for <span class="math inline">\(\mu\)</span> for which <span class="math inline">\(p &gt; \alpha\)</span>. Such an interval is called a <span class="math inline">\((1-\alpha) \times 100\%\)</span> <em>confidence interval</em>.</p>
<p>In what follows, we bring back our sample of <span class="math inline">\(N=15\)</span> from earlier and compute confidence intervals using the corresponding sampling distribution of <span class="math inline">\(m\)</span>. We will use <span class="math inline">\(\alpha = 0.05\)</span>, i.e. we will compute 95% confidence intervals. We will compute three types of 95% confidence intervals: based on right-sided, left-sided, and two-sided <span class="math inline">\(p\)</span>-values.</p>
<div id="computing-and-interpreting-the-ci-based-on-the-right-sided-p-value" class="section level3">
<h3>Computing and interpreting the CI based on the right-sided <span class="math inline">\(p\)</span>-value</h3>
<p>The top two panels show our building blocks for computing the <span class="math inline">\(p\)</span>-value (nothing new here).</p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/rightsided%20alpha%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>The bottom two panels show how we compute the confidence interval using the right-sided <span class="math inline">\(p\)</span>-value, which includes all hypothetical values for <span class="math inline">\(\mu\)</span> for which <span class="math inline">\(p &gt; \alpha\)</span>. All we have to do is find the hypothetical values for <span class="math inline">\(\mu\)</span> with <span class="math inline">\(p = \alpha\)</span>, which is the boundary. The interval gives us a lower bound for non-rejected values for <span class="math inline">\(\mu\)</span> (and goes up to <span class="math inline">\(\infty\)</span>).</p>
</div>
<div id="computing-and-interpreting-the-ci-based-on-the-left-sided-p-value" class="section level3">
<h3>Computing and interpreting the CI based on the left-sided <span class="math inline">\(p\)</span>-value</h3>
<p>The top two panels show our building blocks for computing the <span class="math inline">\(p\)</span>-value (nothing new here).</p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/leftsided%20alpha%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>The bottom two panels show how we compute the confidence interval using the right-sided <span class="math inline">\(p\)</span>-value, which includes all hypothetical values for <span class="math inline">\(\mu\)</span> for which <span class="math inline">\(p &gt; \alpha\)</span>. All we have to do is find the hypothetical value for <span class="math inline">\(\mu\)</span> with <span class="math inline">\(p = \alpha\)</span>, which is the boundary. The interval gives us an upper bound for non-rejected values for <span class="math inline">\(\mu\)</span> (and goes down to <span class="math inline">\(-\infty\)</span>).</p>
</div>
<div id="computing-and-interpreting-the-ci-based-on-the-two-sided-p-value" class="section level3">
<h3>Computing and interpreting the CI based on the two-sided <span class="math inline">\(p\)</span>-value</h3>
<p>The top two panels show our building blocks for computing the <span class="math inline">\(p\)</span>-value (nothing new here).</p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/twosided%20alpha%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>The bottom two panels show how we compute the confidence interval using the right-sided <span class="math inline">\(p\)</span>-value, which includes all hypothetical values for <span class="math inline">\(\mu\)</span> for which <span class="math inline">\(p &gt; \alpha\)</span>. All we have to do is find the hypothetical values for <span class="math inline">\(\mu\)</span> with <span class="math inline">\(p = \alpha\)</span>, which are the boundaries. The interval gives us a lower and an upper bound for non-rejected values for <span class="math inline">\(\mu\)</span>.</p>
</div>
<div id="interpreting-the-ci" class="section level3">
<h3>Interpreting the CI</h3>
<p>The <span class="math inline">\((1-\alpha) \times 100\%\)</span> confidence interval includes all hypothetical values of <span class="math inline">\(\mu\)</span> for which <span class="math inline">\(p&gt;\alpha\)</span>, and excludes all values for which <span class="math inline">\(p\le\alpha\)</span>. Hence, the <span class="math inline">\((1-\alpha) \times 100\%\)</span> confidence interval covers the true value of <span class="math inline">\(\mu\)</span> <span class="math inline">\(\alpha\times100\%\)</span> of the time in the long-run.</p>
</div>
<div id="avoid-misinterpreting-the-ci" class="section level3">
<h3>Avoid misinterpreting the CI</h3>
<p>A common mistake is to say that any <em>specific</em> confidence interval <em>you calculated</em> – e.g. the one in the plot above – covers the true value of <span class="math inline">\(\mu\)</span> with probability <span class="math inline">\(1-\alpha\)</span>. Once again, this statement raises a red flag from earlier: We would be assigning probability to a non-repeatable event, which would be inconsistent with the frequentist interpretation of probability! The key part of the above statement is “<span class="math inline">\(\mu\)</span> lies within this interval, which is just as non-repeatable as”time-travel is possible".</p>
<p>The correct statement is that confidence intervals will cover the true value of <span class="math inline">\(\mu\)</span> at a rate <span class="math inline">\(1-\alpha\)</span>, in the long-run. This does <em>not</em> mean there is a <span class="math inline">\(1-\alpha\)</span> probability that our <em>specific</em> confidence interval covers the true value of <span class="math inline">\(\mu\)</span>.</p>
<p>When I first learned about this I was surprised, because intuitively it felt like the second statement follows from the first. It does not! But I think the reason it felt that way was that in everyday language, we use the word ‘probability’ loosely without qualifying what it acutally refers to. I know that if I flip a coin over and over again, I will get heads and tails 50% of the time each. So intuitively, if you flipped a coin and I were to predict the outcome, I would put equal credences on heads and tails. These are two different statements: one about long-run frequencies, the other about personal belief. Yet both are saying that getting heads and tails have equal probability, which makes it feel like they go hand in hand.</p>
<p>To really drive this point home, consider the following example. We go back to our working scenario where we are after <span class="math inline">\(\mu\)</span>, but we use a different method to obtain our interval: we roll three dice, if all three show the same number the interval is empty (~ 3% of the time), if not the interval goes from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>. This is a ~97% confidence interval in the sense that it will cover <span class="math inline">\(\mu\)</span> ~97% of the time in the long-run. Yet you would never even think about saying that any <em>specific</em> interval has a 97% probability of covering <span class="math inline">\(\mu\)</span> – if it’s infinite it clearly does, and if it’s empty it clearly doesn’t. The 97% are really a statement about the method of obtaining intervals (and its stable long-run performance), not about any particular interval itself (and the data you happened to get).</p>
<p>Note that another common mistake is to say that the confidence interval includes a range of ‘plausible’ values for <span class="math inline">\(\mu\)</span>. This is pretty much the same fallacy as discussed above (‘plausible’ means the same as ‘probable’), just without attaching a number to it.</p>
<p>To sum up, <span class="math inline">\(1-\alpha\)</span> is not the probability that any specific confidence interval covers the true value of <span class="math inline">\(\mu\)</span> – it is the rate at which confidence intervals cover the true value of <span class="math inline">\(\mu\)</span> in the long-run.</p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/overview%20plot%20hyptesting-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/rightsided%20beta%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/leftsided%20beta%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/twosided%20beta%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/rightsided%20inference%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/leftsided%20inference%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/twosided%20inference%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/power%20vs%20N-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/power%20vs%20alpha-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/power%20contour-1.png" width="624" style="display: block; margin: auto;" /></p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/repeat%20sampdist%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/z-statistic%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/t-statistic%20plot-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/CI%20plot%20zstat-1.png" width="691.2" style="display: block; margin: auto;" /></p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/CI%20plot%20tstat-1.png" width="691.2" style="display: block; margin: auto;" /></p>
<p><img src="/post/2020-12-06-freqstats-1_files/figure-html/power%20contour%20d-1.png" width="624" style="display: block; margin: auto;" /></p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Note: In our working scenario, the shape of the sampling distribution is independent of the true value of <span class="math inline">\(\mu\)</span>. In contrast, in the urn-example, the shape of the sampling distribution depends on the true ratio of red/blue. You can plot the sampling distribution using the following <code>R</code> code: <code>plot(0:N/N, dbinom(0:N,N,prop), type="h", xlab="Proportion Red", ylab="Frequency")</code>. Just set <code>N</code> as the number of pills you grab (in our example, <code>N=10</code>) and <code>prop</code> as the proportion of red pills in the urn (a number between <code>0</code> and <code>1</code>). If the true ratio is 50%/50% (<code>prop=0.5</code>), the distribution will be symmetric. However, if the true proportion is low – e.g. 90%/10% (<code>prop=0.9</code>) – or high – e.g. 10%/90% (<code>prop=0.1</code>) – the distribution will be skewed. As <code>N</code> increases, this skewness goes away. Can you tell why? Note that for any given value of <code>N</code>, the distribution will have the highest variance if <code>prop=0.5</code>.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
</div>

    
    
    

    
    
        <h4 class="page-header">Comments</h4>
        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "username" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    

</main>

        <footer>
            <p class="copyright text-muted">© All rights reserved. Powered by <a href="https://gohugo.io">Hugo</a> and <a href="https://github.com/calintat/minimal">Minimal</a>.</p>
        </footer>

        

        
    </body>

</html>

